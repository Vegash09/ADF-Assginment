Sales View Data Engineering Project
ðŸ“Œ Overview

This project builds an end-to-end data pipeline using Azure Data Factory (ADF), Azure Data Lake Storage (ADLS), and Databricks.
The pipeline ingests raw CSV files, processes them through the Bronze â†’ Silver â†’ Gold architecture, and prepares curated data for analytics.

ðŸš€ Workflow
1. Data Ingestion (ADF + ADLS)

Created an ADLS container â†’ sales_view_devtst.

Folders created: customer, product, store, sales.

Uploaded CSV files into each folder.

Built an ADF pipeline to:

Identify the latest modified file from each folder.

Copy the latest file into the Bronze layer (bronze/sales_view/{folder}).

Used Foreach activity with variables:

latestFile â†’ stores the most recently modified file.

referenceDate â†’ initialized to 1900-01-01 00:00:00 for comparison.

2. Bronze â†’ Silver Transformations (Databricks)
Customer

Converted headers â†’ snake_case (UDF).

Split name â†’ first_name, last_name.

Extracted domain from email.

Mapped gender â†’ M/F.

Split joining_date into date (yyyy-MM-dd) and time.

Derived expenditure_status â†’ (MINIMUM if spent < 200 else MAXIMUM).

Upserted into â†’ silver/sales_view/customer.

Product

Converted headers â†’ snake_case.

Added sub_category (mapping category_id â†’ category).

Upserted into â†’ silver/sales_view/product.

Store

Converted headers â†’ snake_case.

Extracted store_category from email domain.

Formatted created_at & updated_at â†’ yyyy-MM-dd.

Upserted into â†’ silver/sales_view/store.

Sales

Converted headers â†’ snake_case.

Upserted into â†’ silver/sales_view/customer_sales.

3. Silver â†’ Gold Transformation

Joined product and store tables â†’ selected attributes such as store_name, location, product_name, price, stock_quantity, image_url etc.
