{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "339b2868-f2d4-4d14-bb74-e6b8932bddea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utils File Setup  \n",
    "\n",
    "- Created a separate **utils.py** file to store all reusable functions.  \n",
    "- Implemented a function to convert **camelCase / PascalCase** column names into **snake_case** dynamically.  \n",
    "- All transformation logic (column renaming, UDFs, helper functions) is imported from this utils file.  \n",
    "- This keeps the notebook clean and ensures reusability across Customer, Product, Store, and Sales transformations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2a6666-e26c-4413-8a7b-4387918b1ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "#Converting from camelCase or PascalCase to snake_case\n",
    "def to_snake_case(name: str) -> str:\n",
    "    name = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name) \n",
    "    name = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r\"\\1_\\2\", name)  \n",
    "    return name.lower()\n",
    "\n",
    "#Cleaning and renaming all column names to snake_case using df.toDF()\n",
    "def clean_and_snake_case_columns(df: DataFrame) -> DataFrame:\n",
    "    cleaned_cols = []\n",
    "    for col_name in df.columns:\n",
    "        cleaned_name = re.sub(r\"[ (){};\\n\\t=]\", \"\", col_name).strip().replace(\" \", \"_\")\n",
    "        snake_case_name = to_snake_case(cleaned_name)\n",
    "        cleaned_cols.append(snake_case_name)\n",
    "    return df.toDF(*cleaned_cols)\n",
    "\n",
    "#Reading delta table and cleaning and snake_case the column names\n",
    "def read_delta_with_snake_case(spark, path: str) -> DataFrame:\n",
    "    df = spark.read.format(\"delta\").load(path)\n",
    "    return clean_and_snake_case_columns(df)\n",
    "\n",
    "# Joining the store and product on store_id\n",
    "def get_store_product_data(product_df: DataFrame, store_df: DataFrame) -> DataFrame:\n",
    "    return store_df.join(product_df, on=\"store_id\", how=\"inner\")\n",
    "\n",
    "# Joining the sales with enriched store-product data on product_id\n",
    "def enrich_sales_with_store_product(sales_df: DataFrame, store_product_df: DataFrame) -> DataFrame:\n",
    "    return sales_df.join(store_product_df, on=\"product_id\", how=\"inner\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
