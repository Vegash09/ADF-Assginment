{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d37c067-c024-4b28-aee8-2ec86452bcf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration: ADF to Databricks using SAS Token  \n",
    "\n",
    "- Created a container in **ADLS** for project data storage.  \n",
    "- Configured **SAS Token** authentication to connect Databricks with ADLS.  \n",
    "- Mounted the container in Databricks using SAS Token for secure access.  \n",
    "- Verified the connection by listing files and checking read/write operations.  \n",
    "\n",
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df4fe3d-4f0a-42c4-85c4-3e3169a1b26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "  \"fs.azure.account.auth.type.stassignment.dfs.core.windows.net\",\n",
    "  \"SAS\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.sas.token.provider.type.stassignment.dfs.core.windows.net\",\n",
    "  \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.sas.fixed.token.stassignment.dfs.core.windows.net\",\n",
    "  \"sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-09-16T22:03:31Z&st=2025-08-27T13:48:31Z&spr=https&sig=jR%2FHBfVXjYTnv5aPFEkF5Pa9K4GxXewiK5NhxacxNHQ%3D\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a8ccde-944b-4bdd-9fcd-4a6c2a9f11fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_base_path = \"abfss://bronze@stassignment.dfs.core.windows.net/\"\n",
    "silver_base_path = \"abfss://silver@stassignment.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6866b35-2d4a-4b02-9424-e644c6bb8450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run /Workspace/adf_assignment/src/bronze_to_silver/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9263d9b-d911-47b8-92bc-792b0c5ccd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze to Silver Transformation\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Customer File  \n",
    "\n",
    "- Convert all column headers to **snake_case** in lower case (UDF function).  \n",
    "- Split `\"Name\"` column → `first_name`, `last_name`.  \n",
    "- Create column `domain` → extract from `email`.  \n",
    "- Create column `gender` → `M` for Male, `F` for Female.  \n",
    "- From `joining_date` → split into `date` (`yyyy-MM-dd` format) and `time`.  \n",
    "- Create column `expenditure_status` → if `spent < 200` then `MINIMUM` else `MAXIMUM`.  \n",
    "- Write based on **upsert** → `table_name: customer`  \n",
    "  Path: `silver/sales_view/customer/{delta parquet}`  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fe4ed3-035d-4a33-8bf6-27752a21012b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, when, to_date\n",
    "\n",
    "# Step 1: Read CSV from Bronze Layer\n",
    "df_customer = spark.read.option(\"header\", True).format(\"csv\").load(f\"{bronze_base_path}/Customer/\")\n",
    "display(df_customer)\n",
    "\n",
    "# Step 2: Clean and Rename Columns\n",
    "df_customer = clean_and_snake_case_columns(df_customer)\n",
    "df_customer.printSchema()\n",
    "display(df_customer)\n",
    "\n",
    "# Step 3: Extract first_name and last_name from 'name'\n",
    "df_customer = df_customer.withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0)) \\\n",
    "                         .withColumn(\"last_name\", split(col(\"name\"), \" \").getItem(1))\n",
    "\n",
    "# Step 4: Extract domain from 'email_id'\n",
    "df_customer = df_customer.withColumn(\"domain\", split(col(\"email_id\"), \"@\").getItem(1)) \\\n",
    "                         .withColumn(\"domain\", split(col(\"domain\"), r\"\\.\").getItem(0))\n",
    "\n",
    "# Step 5: Convert gender values to M/F/O\n",
    "df_customer = df_customer.withColumn(\"gender\", when(col(\"gender\") == \"male\", \"M\")\n",
    "                                               .when(col(\"gender\") == \"female\", \"F\")\n",
    "                                               .otherwise(\"O\"))\n",
    "\n",
    "# Step 6: Split joining_date into date and time using try_to_date\n",
    "df_customer = df_customer.withColumn(\"joining_date_split\", split(col(\"joining_date\"), \" \")) \\\n",
    "                         .withColumn(\"time\", col(\"joining_date_split\").getItem(1)) \\\n",
    "                         .drop(\"joining_date_split\")\n",
    "\n",
    "# Step 7: Create expenditure_status based on 'spent'\n",
    "df_customer = df_customer.withColumn(\"spent_numeric\", col(\"spent\").cast(\"double\")) \\\n",
    "                         .withColumn(\"expenditure_status\", \n",
    "                                     when(col(\"spent_numeric\") < 200, \"MINIMUM\")\n",
    "                                     .otherwise(\"MAXIMUM\")) \\\n",
    "                         .drop(\"spent_numeric\")\n",
    "\n",
    "display(df_customer)\n",
    "\n",
    "# Step 8: Write to Silver Layer as Delta Table\n",
    "df_customer.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/customer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c719adeb-a738-4d7e-9ad7-8f611d207fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) Product File  \n",
    "\n",
    "- Convert all column headers to **snake_case** in lower case (UDF function).  \n",
    "- Create column `sub_category` from `category_id`:  \n",
    "  - 1 → phone  \n",
    "  - 2 → laptop  \n",
    "  - 3 → playstation  \n",
    "  - 4 → e-device  \n",
    "- Write based on **upsert** → `table_name: product`  \n",
    "  Path: `silver/sales_view/product/{delta parquet}`  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36bca42-82f7-44ac-81e9-fb369426a261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Read CSV from Bronze layer\n",
    "df_product = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/Product\")\n",
    "display(df_product)\n",
    "\n",
    "# Clean and convert column names to snake_case\n",
    "df_product = clean_and_snake_case_columns(df_product)\n",
    "\n",
    "# Add sub_category column based on category_id\n",
    "df_product = df_product.withColumn(\"sub_category\", \n",
    "    when(col(\"category_id\") == 1, \"phone\")\n",
    "   .when(col(\"category_id\") == 2, \"laptop\")\n",
    "   .when(col(\"category_id\") == 3, \"playstation\")\n",
    "   .when(col(\"category_id\") == 4, \"e-device\")\n",
    ")\n",
    "\n",
    "display(df_product)\n",
    "\n",
    "# Write to Silver layer as Delta table\n",
    "df_product.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28db50b3-4014-4e97-ac77-ec8cfd572557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3) Store File  \n",
    "\n",
    "- Convert all column headers to **snake_case** in lower case (UDF function).  \n",
    "- Create column `store_category` from email domain (e.g., `johndoe@electromart.com` → `electromart`).  \n",
    "- Format `created_at`, `updated_at` columns → `yyyy-MM-dd`.  \n",
    "- Write based on **upsert** → `table_name: store`  \n",
    "  Path: `silver/sales_view/store/{delta parquet}`  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999d1251-8c5b-4d72-bc17-5c06dda04905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, regexp_extract, to_date, when\n",
    "\n",
    "# Step 1: Reading CSV from Bronze\n",
    "df_store = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/Store\")\n",
    "df_store.display()\n",
    "\n",
    "# Step 2: Cleaning and renaming columns to snake_case\n",
    "df_store = clean_and_snake_case_columns(df_store)\n",
    "\n",
    "# Step 3: Extracting store_category from email domain\n",
    "df_store = df_store.withColumn(\n",
    "    \"store_category\",\n",
    "    split(split(col(\"email_address\"), \"@\").getItem(1), r\"\\.\").getItem(0)\n",
    ")\n",
    "\n",
    "# Step 4: Defining regex pattern for 'dd-MM-yyyy'\n",
    "date_pattern = r\"^\\d{2}-\\d{2}-\\d{4}$\"\n",
    "\n",
    "# Step 5: Converting `created_at` only if it matches the date pattern\n",
    "df_store = df_store.withColumn(\n",
    "    \"created_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"created_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"created_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Step 6: Converting `updated_at` only if it matches the date pattern\n",
    "df_store = df_store.withColumn(\n",
    "    \"updated_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"updated_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"updated_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_store.display()\n",
    "\n",
    "# Step 7: Write to Silver Layer as Delta table\n",
    "df_store.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{silver_base_path}/store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186cc471-91c5-4bc7-b3cc-73fea18aadd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4) Sales File  \n",
    "\n",
    "- Convert all column headers to **snake_case** in lower case (UDF function).  \n",
    "- Write based on **upsert** → `table_name: customer_sales`  \n",
    "  Path: `silver/sales_view/customer_sales/{delta parquet}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c531debf-f892-4121-a061-536cf2c1c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Step 1: Read CSV from Bronze Layer\n",
    "df_sales = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/Sales\")\n",
    "display(df_sales) \n",
    "\n",
    "# Step 2: Clean and convert column names to snake_case\n",
    "df_sales = clean_and_snake_case_columns(df_sales)\n",
    "display(df_sales)\n",
    "\n",
    "# Step 3: Format timestamp columns\n",
    "df_sales = df_sales.withColumn(\n",
    "    \"order_date\", to_timestamp(col(\"order_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ").withColumn(\n",
    "    \"ship_date\", to_timestamp(col(\"ship_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ")\n",
    "\n",
    "# Step 4: Write to Silver Layer in Delta format\n",
    "df_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"{silver_base_path}/customer_sales\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "drivers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
