{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49c4a83f-8ff7-4a5b-a6b9-9bc802437ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration: ADF to Databricks using SAS Token  \n",
    "\n",
    "- Created a container in **ADLS** for project data storage.  \n",
    "- Configured **SAS Token** authentication to connect Databricks with ADLS.  \n",
    "- Mounted the container in Databricks using SAS Token for secure access.  \n",
    "- Verified the connection by listing files and checking read/write operations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7832857d-990f-41e5-86d8-6fcb437282ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "  \"fs.azure.account.auth.type.stassignment.dfs.core.windows.net\",\n",
    "  \"SAS\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.sas.token.provider.type.stassignment.dfs.core.windows.net\",\n",
    "  \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.sas.fixed.token.stassignment.dfs.core.windows.net\",\n",
    "  \"sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-09-16T22:03:31Z&st=2025-08-27T13:48:31Z&spr=https&sig=jR%2FHBfVXjYTnv5aPFEkF5Pa9K4GxXewiK5NhxacxNHQ%3D\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20007dca-68dd-4cbe-abc8-8f48a0a080a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run /Workspace/adf_assignment/src/bronze_to_silver/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ae5a1e-f9f3-4927-8290-e30a42235cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gold Layer  \n",
    "\n",
    "- Joined **Customer, Product, Store, Sales** tables from Silver.  \n",
    "- Created fact & dimension style tables for analytics.  \n",
    "- Applied aggregations (sales per customer, store, product, etc.).  \n",
    "- Standardized date format (`yyyy-MM-dd`).  \n",
    "- Written to: `gold/sales_view/{tablename}/{delta parquet}`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993b7a1d-1915-4256-ba04-d1dfaac453bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_base_path = \"abfss://silver@stassignment.dfs.core.windows.net/\"\n",
    "gold_output_path = \"abfss://gold@stassignment.dfs.core.windows.net/StoreProductSalesAnalysis\"\n",
    "\n",
    "sales_df = read_delta_with_snake_case(spark, f\"{silver_base_path}/customer_sales\")\n",
    "product_df = read_delta_with_snake_case(spark, f\"{silver_base_path}/product\")\n",
    "store_df = read_delta_with_snake_case(spark, f\"{silver_base_path}/store\")\n",
    "\n",
    "sales_df = sales_df.withColumnRenamed(\"product__id\", \"product_id\")\n",
    "\n",
    "store_product_df = get_store_product_data(product_df, store_df)\n",
    "\n",
    "result_df = enrich_sales_with_store_product(sales_df, store_product_df)\n",
    "\n",
    "duplicate_cols = [col_name for col_name in result_df.columns if result_df.columns.count(col_name) > 1]\n",
    "if duplicate_cols:\n",
    "    print(f\"Duplicate Columns Detected: {duplicate_cols}\")\n",
    "\n",
    "result_df = result_df.drop(*set(duplicate_cols[1:]))\n",
    "\n",
    "selected_cols = [\n",
    "    \"order_date\", \"category\", \"city\", \"customer_id\", \"order_id\", \"product_id\", \"profit\", \"region\", \"sales\", \"segment\",\n",
    "    \"ship_date\", \"ship_mode\", \"latitude\", \"longitude\",\n",
    "    \"store_name\", \"location\", \"manager_name\", \"product_name\", \"price\", \"stock_quantity\", \"image_url\"\n",
    "]\n",
    "\n",
    "selected_cols = [col for col in selected_cols if col in result_df.columns]\n",
    "\n",
    "result_df = result_df.select(*selected_cols)\n",
    "result_df.display()\n",
    "\n",
    "result_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(gold_output_path)\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS StoreProductSalesAnalysis\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE StoreProductSalesAnalysis\n",
    "    USING DELTA\n",
    "    LOCATION '{gold_output_path}'\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "drivers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
